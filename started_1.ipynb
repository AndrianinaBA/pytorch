{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06ca4a91-7650-4b9e-bd8a-350bc4035ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311d0a36-4932-4004-878c-d240c9347b1c",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "\n",
    "The aurograde package provides automatic differentitation on all aperations on tensores. Generally speaking, torch.autograd is an engine for computing the vector Jacobian product. It computespartial derivatives while applying the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69bf0f32-d39b-4b3f-87c2-c0129b536f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9588f38e-e1d0-4769-9390-d1b3d3deb50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6268, 0.5272, 0.8882], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fed82a72-eb5e-4f06-bc1d-d97ff12d6fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x74e1955e9a80>\n"
     ]
    }
   ],
   "source": [
    "y = x+2\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8792f733-ab82-412c-827f-fecf2291e542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20.7004, 19.1608, 25.0259], grad_fn=<MulBackward0>)\n",
      "tensor(21.6290, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y*y*3\n",
    "print(z)\n",
    "\n",
    "z = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16bfe9b1-89f1-407a-98ad-cdbe514a87ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.2536, 5.0545, 5.7765])\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "\n",
    "print(x.grad) #dz/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2df84ef0-628f-4c09-9918-1ff08b6305d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Careful : backward() accumulates the gradient for this tensor into .grad attribute.\n",
    "#call optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1eeed6-efe8-48eb-9817-76df810a429b",
   "metadata": {},
   "source": [
    "#### Stop a tensor from trcking history\n",
    "\n",
    "For example during the training loop when we want to update our weights, or after training during evaluation. These operations should not be part of the gradient computation. To prevent this, we can use : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62959b84-535b-4187-b6f6-078d4fcfddb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1002, -0.1635,  0.4429],\n",
       "        [ 1.2792, -1.0479, -0.5346]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7804636-2dd9-41c9-8aaf-5a22677dd3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "b = (a*a).sum()\n",
    "print(a.requires_grad)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa25d1e0-0fa7-4439-bb9e-538c78797317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "<SumBackward0 object at 0x74e1955e9870>\n"
     ]
    }
   ],
   "source": [
    "a.requires_grad_(True)\n",
    "b = (a*a).sum()\n",
    "print(a.requires_grad)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "497b6a5f-0c99-4b3e-8db4-9638dac706db",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2,3,requires_grad=True)\n",
    "b = a.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8846710-7dc1-42d1-80f5-5f3750507779",
   "metadata": {},
   "source": [
    "### Gradient Descent Approach\n",
    "Linear regression example\n",
    "\n",
    "f(x) = w*x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03422e66-1797-4567-bb7a-723cc0d976af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([1,2,3,4,5,6,7,8],dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8,10,12,14,16])\n",
    "\n",
    "w = torch.tensor(0.0,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91f8bc6e-08c9-4487-83f7-528ee72de456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model output\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "#loss = MSE\n",
    "def loss(y,y_pred):\n",
    "    return ((y_pred-y)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "730406c4-4971-42b2-b886-7784409dbbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training : f(5.0) = 0.000\n"
     ]
    }
   ],
   "source": [
    "X_test = 5.0\n",
    "print(f'Prediction before training : f({X_test}) = {forward(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03c26426-ef64-4adc-9e55-cc83e9c7f43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 : w = 2.000,loss = 0.000\n",
      "epoch 20 : w = 2.000,loss = 0.000\n",
      "epoch 30 : w = 2.000,loss = 0.000\n",
      "epoch 40 : w = 2.000,loss = 0.000\n",
      "epoch 50 : w = 2.000,loss = 0.000\n",
      "epoch 60 : w = 2.000,loss = 0.000\n",
      "epoch 70 : w = 2.000,loss = 0.000\n",
      "epoch 80 : w = 2.000,loss = 0.000\n",
      "epoch 90 : w = 2.000,loss = 0.000\n",
      "epoch 100 : w = 2.000,loss = 0.000\n",
      "Prediction after training :f(5.0) = 10.000\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "lr = 0.01\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    #predict = forward pass\n",
    "    y_pred=forward(X)\n",
    "\n",
    "    #loss\n",
    "    l = loss(Y,y_pred)\n",
    "\n",
    "    #calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    #update weights\n",
    "    #w.data = w.data - lr*w.grad\n",
    "    with torch.no_grad():\n",
    "        w -= lr*w.grad\n",
    "\n",
    "    #zero teh gradients after updating\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'epoch {epoch+1} : w = {w.item():.3f},loss = {l.item():.3f}')\n",
    "\n",
    "print(f'Prediction after training :f({X_test}) = {forward(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b743d09-7367-49d9-a811-97c8f17f287d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
